{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkapobel/Algo3-2c-2016/blob/master/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt7r2gq3tTMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "258b4539-99c6-41b7-9f32-9e47c399716f"
      },
      "source": [
        "!pip3 install tensorboardX\n",
        "!pip3 install torchtext==0.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (42.0.2)\n",
            "Requirement already satisfied: torchtext==0.4 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yt9726HI9W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import sqlite3\n",
        "import string\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import torchtext.data as data\n",
        "import torchtext.datasets as datasets\n",
        "import torchtext.vocab as vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset, RandomSampler, random_split\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from tensorboardX import SummaryWriter\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBF_ZW9wKyz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mZ1Qprt8i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')\n",
        "%cd 'gdrive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHYZCY_h1znl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfW4HAhUIBbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_dict = pd.read_csv(filepath_or_buffer='glove.840B.300d-char.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
        "                        usecols=[0]).values\n",
        "char_dict = [char[0] for char in char_dict]\n",
        "\n",
        "dict = pd.read_csv(filepath_or_buffer='glove.840B.300d-char.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n",
        "dict_len, embed_size = dict.shape\n",
        "dict_len += 1\n",
        "unknown_char = np.zeros((1, embed_size))\n",
        "dict = torch.from_numpy(np.concatenate([unknown_char, dict], axis=0).astype(np.float))\n",
        "\n",
        "dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R82g2aiOt-Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vocab(texts_path='texts.pt', word2vec_file='glove.6B.50d',vocab_path=None):\n",
        "  texts = torch.load(texts_path)\n",
        "  counter = Counter()\n",
        "  for document in texts:\n",
        "    for sent in document:\n",
        "      counter.update(sent)\n",
        "  vocabulary = vocab.Vocab(counter, vectors=[word2vec_file])\n",
        "  torch.save(vocabulary, vocab_path)\n",
        "\n",
        "def create_texts(data_path='train.csv', texts_path=None):\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "  stoplist =  stopwords.words('english') + list(string.punctuation)\n",
        "  texts = []\n",
        "  with open(data_path) as csv_file:\n",
        "    reader = csv.reader(csv_file, quotechar='\"')\n",
        "    for line in reader:\n",
        "      text = ' '.join(line[1:]).lower()\n",
        "      #if word not in stoplist\n",
        "      texts.append([[word for word in word_tokenize(s)] for s in sent_tokenize(text)])\n",
        "  torch.save(texts,texts_path)\n",
        "\n",
        "def create_vocabularies():\n",
        "  create_texts('ag_news_csv/train.csv', 'ag_news_csv/train_texts.pt')\n",
        "  create_vocab('ag_news_csv/train_texts.pt','glove.6B.50d','ag_news_csv/train_vocab.pt')\n",
        "  create_texts('ag_news_csv/test.csv', 'ag_news_csv/test_texts.pt')\n",
        "  create_vocab('ag_news_csv/test_texts.pt','glove.6B.50d','ag_news_csv/test_vocab.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhxlTPAKLhza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create_vocabularies()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcGPX6kruF46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        data_path,\n",
        "        use_char_embeddings,\n",
        "        char2vec_file,\n",
        "        word2vec_file,\n",
        "        max_length_char=20,\n",
        "        max_length_word=25,\n",
        "        max_length_sentences=12, \n",
        "        dataset_name='dataset', \n",
        "        min_freq=6, \n",
        "        pre_vocab='vocab.pt'):\n",
        "        super(MyDataset, self).__init__()\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        self.dataset_name = dataset_name\n",
        "        self.con = sqlite3.Connection(':memory:')\n",
        "        self.cur = self.con.cursor()\n",
        "        self.cur.execute('DROP TABLE IF EXISTS \"texts_{}\"'.format(dataset_name))\n",
        "        self.cur.execute('CREATE TABLE IF NOT EXISTS \"texts_{}\" (\"id\" int, \"text\" varchar({}), \"label\" int(2), PRIMARY KEY (\"id\"));'.format(self.dataset_name, max_length_sentences))\n",
        "        with open(data_path) as csv_file:\n",
        "            reader = csv.reader(csv_file, quotechar='\"')\n",
        "            index = 0\n",
        "            for line in reader:\n",
        "                text = ' '.join(line[1:]).lower()\n",
        "                label = int(line[0]) - 1\n",
        "                self.cur.execute('INSERT INTO \"texts_{}\" (id, text, label) VALUES (?, ?, ?)'.format(dataset_name), [index, text, label])\n",
        "                index += 1\n",
        "        self.con.commit()\n",
        "        self.max_length_sentences = max_length_sentences\n",
        "        self.max_length_word = max_length_word\n",
        "        self.max_length_char = max_length_char\n",
        "        self.num_classes = self.cur.execute('SELECT COUNT(DISTINCT label) FROM \"texts_{}\"'.format(self.dataset_name)).fetchone()[0]\n",
        "        if use_char_embeddings == True:\n",
        "            self.create_char_dict(char2vec_file)\n",
        "        else:\n",
        "            self.create_word_vocab(pre_vocab, word2vec_file)           \n",
        "                \n",
        "    def __len__(self):\n",
        "        return self.cur.execute('SELECT COUNT(*) FROM \"texts_{}\"'.format(self.dataset_name)).fetchone()[0]\n",
        "\n",
        "    def get_target_count(self, target):\n",
        "        return self.cur.execute('SELECT COUNT(*) FROM \"texts_{}\" WHERE label = {}'.format(self.dataset_name, target)).fetchone()[0]\n",
        "\n",
        "    def get_texts(self, indexes=[]):\n",
        "        if len(indexes):\n",
        "            return self.cur.execute('SELECT text FROM texts_{} WHERE id IN ({})'.format(self.dataset_name, ', '.join(indexes)))\n",
        "        return self.cur.execute('SELECT text FROM texts_{}'.format(self.dataset_name))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text, label = self.cur.execute('SELECT text, label FROM texts_{} WHERE id = {}'.format(self.dataset_name, index)).fetchone()\n",
        "        if self.use_char_embeddings == True:\n",
        "            return self.create_char_encodes(text, label)\n",
        "        return self.create_word_encodes(text, label)\n",
        "\n",
        "    def create_char_encodes(self, text, label):\n",
        "        document_encode = [\n",
        "            [self.generate_char_list(word) for word in word_tokenize(text=sentences)] for sentences\n",
        "            in\n",
        "            sent_tokenize(text=text)]\n",
        "        for sentences in document_encode:\n",
        "            if len(sentences) < self.max_length_word:\n",
        "                extended_words = [self.get_char_list_extend(self.max_length_char) for _ in range(self.max_length_word - len(sentences))]\n",
        "                sentences.extend(extended_words)\n",
        "\n",
        "        if len(document_encode) < self.max_length_sentences:\n",
        "            extended_sentences = [[self.get_char_list_extend(self.max_length_char) for _ in range(self.max_length_word)] for _ in\n",
        "                                  range(self.max_length_sentences - len(document_encode))]\n",
        "            document_encode.extend(extended_sentences)\n",
        "\n",
        "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
        "                          :self.max_length_sentences]\n",
        "\n",
        "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
        "        document_encode += 1\n",
        "\n",
        "        return document_encode.astype(np.int64), label\n",
        "\n",
        "    def create_word_encodes(self, text, label):\n",
        "        #stoplist = stopwords.words('english') + list(string.punctuation) #(word not in stoplist) and \n",
        "        document_encode = [[self.vocabulary.stoi[word] for word in word_tokenize(s) if (word in self.vocabulary.stoi)] for s in sent_tokenize(text)]\n",
        "      \n",
        "        for sentences in document_encode:\n",
        "            if len(sentences) < self.max_length_word:\n",
        "                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n",
        "                sentences.extend(extended_words)\n",
        "\n",
        "        if len(document_encode) < self.max_length_sentences:\n",
        "            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n",
        "                                  range(self.max_length_sentences - len(document_encode))]\n",
        "            document_encode.extend(extended_sentences)\n",
        "\n",
        "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
        "                          :self.max_length_sentences]\n",
        "\n",
        "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
        "        document_encode += 1\n",
        "\n",
        "        return document_encode.astype(np.int64), label\n",
        "\n",
        "    def generate_char_list(self, word):\n",
        "        char_list = [self.char_dict.index(c) for c in list(word[:self.max_length_char]) if c in self.char_dict]\n",
        "        return char_list + ([] if len(char_list) == self.max_length_char else self.get_char_list_extend(self.max_length_char - len(char_list)))\n",
        "\n",
        "    def get_char_list_extend(self, length):\n",
        "        return [-1 for _ in range(length)]\n",
        "\n",
        "    def create_word_vocabulary(self, pre_vocab, word2vec_file):\n",
        "        if pre_vocab is not None:\n",
        "          self.vocabulary = vocab.Vocab(torch.load(pre_vocab).freqs, vectors = [word2vec_file], min_freq = min_freq)\n",
        "        else:\n",
        "          self.vocabulary = vocab.pretrained_aliases[word2vec_file]()\n",
        "\n",
        "    def get_targets(self, indexes=[]):\n",
        "        if len(indexes):\n",
        "            return self.cur.execute('SELECT label FROM texts_{} WHERE id IN ({})'.format(self.dataset_name, ', '.join(indexes)))\n",
        "        return self.cur.execute('SELECT label FROM texts_{}'.format(self.dataset_name))\n",
        "\n",
        "    def __del__(self):\n",
        "        self.cur.close()\n",
        "        self.con.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pycj8VmruVQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HierAttNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_char_embeddings,\n",
        "        char_hidden_size,\n",
        "        word_hidden_size,\n",
        "        sent_hidden_size,  \n",
        "        batch_size, \n",
        "        num_classes,\n",
        "        pretrained_char2vec_file,\n",
        "        pretrained_word2vec_file,\n",
        "        max_char_length,\n",
        "        max_word_length,\n",
        "        max_sent_length):\n",
        "        super(HierAttNet, self).__init__()\n",
        "        self.use_char_embeddings = use_char_embeddings\n",
        "        self.char_hidden_size = char_hidden_size\n",
        "        self.word_hidden_size = word_hidden_size\n",
        "        self.sent_hidden_size = sent_hidden_size\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sent_length = max_sent_length\n",
        "        self.max_word_length = max_word_length\n",
        "        self.max_char_length = max_char_length\n",
        "        if use_char_embeddings == True:\n",
        "            self.char_net = CharNet(pretrained_char2vec_file, char_hidden_size)\n",
        "        self.word_att_net = WordAttNet(use_char_embeddings, pretrained_word2vec_file, word_hidden_size, char_hidden_size)\n",
        "        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n",
        "        self._init_hidden_state()\n",
        "\n",
        "    def _init_hidden_state(self, last_batch_size=None):\n",
        "        if last_batch_size:\n",
        "            batch_size = last_batch_size\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "        if self.use_char_embeddings:\n",
        "            self.char_hidden_state_h0 = torch.zeros(2, batch_size, self.char_hidden_size)\n",
        "            self.char_hidden_state_c0 = torch.zeros(2, batch_size, self.char_hidden_size)\n",
        "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
        "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
        "        if torch.cuda.is_available():\n",
        "            if self.use_char_embeddings:\n",
        "                self.char_hidden_state_h0 = self.char_hidden_state_h0.cuda()\n",
        "                self.char_hidden_state_c0 = self.char_hidden_state_c0.cuda()\n",
        "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
        "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output_list = []\n",
        "        # input.shape: batch, sents, words, (chars if use_char_embeddings)\n",
        "        if self.use_char_embeddings == True:\n",
        "            input = input.permute(1, 0, 2, 3) # sents, batch, words, chars\n",
        "        else:\n",
        "            input = input.permute(1, 0, 2) # sents, batch, words\n",
        "        for i in input:\n",
        "            if self.use_char_embeddings == True:\n",
        "                '''\n",
        "                ip = i.permute(2, 0, 1) # chars, batch, words\n",
        "                if int(i.sum()) == 0:\n",
        "                  continue\n",
        "                for iip in ip:\n",
        "                    #print(iip, 'iip')\n",
        "                    #print(iip.sum(), 'sum')\n",
        "                    if int(iip.sum()) == 0:\n",
        "                      continue\n",
        "                    output, (self.char_hidden_state_h0, self.char_hidden_state_c0) = self.char_net(iip.permute(1, 0), (self.char_hidden_state_h0, self.char_hidden_state_c0))\n",
        "                    #print(output, 'output')\n",
        "                ip = output\n",
        "                #print(ip.shape, 'ip shape')\n",
        "                #print(ip, 'ip')\n",
        "                #print(0/0)\n",
        "                '''\n",
        "                ip = i.permute(1, 2, 0)\n",
        "                output_list_2 = []\n",
        "                for iip in ip:\n",
        "                  output, (self.char_hidden_state_h0, self.char_hidden_state_c0) = self.char_net(iip, (self.char_hidden_state_h0, self.char_hidden_state_c0))\n",
        "                  #print(output.shape, 'output shape')\n",
        "                  #print(output[-1].shape, 'last')\n",
        "                  output_list_2.append(output[-1][None, :, :])\n",
        "                ip = torch.cat(output_list_2, 0)\n",
        "                #print(ip.shape, 'ip shape')\n",
        "            else:\n",
        "                ip = i.permute(1, 0)\n",
        "                #print(ip.shape, 'ip after word net')\n",
        "            output, self.word_hidden_state = self.word_att_net(ip, self.word_hidden_state)\n",
        "            output_list.append(output)\n",
        "        output = torch.cat(output_list, 0)\n",
        "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vOepBlVuZFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentAttNet(nn.Module):\n",
        "    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n",
        "        super(SentAttNet, self).__init__()\n",
        "\n",
        "        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n",
        "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n",
        "\n",
        "        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n",
        "        # self.sent_softmax = nn.Softmax()\n",
        "        # self.fc_softmax = nn.Softmax()\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "        self.sent_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "        f_output, h_output = self.gru(input, hidden_state)\n",
        "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
        "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
        "        output = F.softmax(output, dim=0)\n",
        "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, h_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTWU0e7iubau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordAttNet(nn.Module):\n",
        "    def __init__(self, use_char_embeddings, word2vec_file, hidden_size=50, char_hidden_size=50):\n",
        "        super(WordAttNet, self).__init__()\n",
        "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
        "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
        "\n",
        "        self.use_char_embeddings = use_char_embeddings\n",
        "        if use_char_embeddings == True:\n",
        "            embed_size = 2 * char_hidden_size\n",
        "        else:\n",
        "            dict = vocab.pretrained_aliases[word2vec_file]()\n",
        "            dict_len, embed_size = len(dict.vectors), dict.dim #len(dict), dict.dim\n",
        "            dict_len += 1\n",
        "            unknown_word = np.zeros((1, embed_size))\n",
        "            dict = torch.from_numpy(np.concatenate([unknown_word, dict.vectors], axis=0).astype(np.float))\n",
        "            self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
        "        \n",
        "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "        self.word_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "        output = self.lookup(input)\n",
        "        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n",
        "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
        "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
        "        output = F.softmax(output, dim=0)\n",
        "        output = element_wise_mul(f_output,output.permute(1,0))\n",
        "\n",
        "        return output, h_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLlz8Za0QXDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharNet(nn.Module):\n",
        "    def __init__(self, char2vec_path, hidden_size=50):\n",
        "        super(CharNet, self).__init__()\n",
        "        dict = self.create_char_dict(char2vec_path)\n",
        "        dict_len, embed_size = dict.shape\n",
        "        dict_len += 1\n",
        "        unknown_char = np.zeros((1, embed_size))\n",
        "        dict = torch.from_numpy(np.concatenate([unknown_char, dict], axis=0).astype(np.float))\n",
        "        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
        "\n",
        "    def create_char_dict(self, char2vec_path):\n",
        "        dict = pd.read_csv(filepath_or_buffer=char2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n",
        "        return dict\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "        output = self.lookup(input)\n",
        "        f_output, hidden_state = self.lstm(output.float(), hidden_state)  # feature output and hidden state output\n",
        "        return f_output, hidden_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azZORZImufP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_evaluation(y_true, y_prob, list_metrics):\n",
        "    y_pred = np.argmax(y_prob, -1)\n",
        "    output = {}\n",
        "    if 'accuracy' in list_metrics:\n",
        "        output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n",
        "    if 'loss' in list_metrics:\n",
        "        try:\n",
        "            output['loss'] = metrics.log_loss(y_true, y_prob)\n",
        "        except ValueError:\n",
        "            output['loss'] = -1\n",
        "    if 'confusion_matrix' in list_metrics:\n",
        "        output['confusion_matrix'] = str(metrics.confusion_matrix(y_true, y_pred))\n",
        "    return output\n",
        "\n",
        "def matrix_mul(input, weight, bias=False):\n",
        "    feature_list = []\n",
        "    for feature in input:\n",
        "        feature = torch.mm(feature, weight)\n",
        "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
        "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
        "        feature = torch.tanh(feature).unsqueeze(0)\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    return torch.cat(feature_list, 0).squeeze()\n",
        "\n",
        "def element_wise_mul(input1, input2):\n",
        "    feature_list = []\n",
        "    for feature_1, feature_2 in zip(input1, input2):\n",
        "        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n",
        "        feature = feature_1 * feature_2\n",
        "        feature_list.append(feature.unsqueeze(0))\n",
        "    output = torch.cat(feature_list, 0)\n",
        "\n",
        "    return torch.sum(output, 0).unsqueeze(0)\n",
        "\n",
        "def get_max_lengths(data_path):\n",
        "    word_length_list = []\n",
        "    sent_length_list = []\n",
        "    with open(data_path) as csv_file:\n",
        "        reader = csv.reader(csv_file, quotechar='\"')\n",
        "        for idx, line in enumerate(reader):\n",
        "            text = \"\"\n",
        "            for tx in line[1:]:\n",
        "                text += tx.lower()\n",
        "                text += \" \"\n",
        "            sent_list = sent_tokenize(text)\n",
        "            sent_length_list.append(len(sent_list))\n",
        "\n",
        "            for sent in sent_list:\n",
        "                word_list = word_tokenize(sent)\n",
        "                word_length_list.append(len(word_list))\n",
        "\n",
        "        sorted_word_length = sorted(word_length_list)\n",
        "        sorted_sent_length = sorted(sent_length_list)\n",
        "\n",
        "    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTBccxIfumBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        \"\"\"Implementation of the model described in the paper: Hierarchical Attention Networks for Document Classification\"\"\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--use_char_embeddings\", type=bool, default=False)\n",
        "    parser.add_argument(\"--data_path\", type=str, default=\"test.csv\")\n",
        "    parser.add_argument(\"--pre_trained_model\", type=str, default=\"trained_models/whole_model_han\")\n",
        "    parser.add_argument(\"--char2vec_file\", type=str, default=\"glove.6B.50d-char.txt\")\n",
        "    parser.add_argument(\"--word2vec_file\", type=str, default='glove.6B.50d')\n",
        "    parser.add_argument(\"--output\", type=str, default=\"predictions\")\n",
        "    parser.add_argument(\"--test_subset_len\", type=int, default=200)\n",
        "    parser.add_argument(\"--min_freq\", type=int, default=6)\n",
        "    parser.add_argument(\"--pre_vocab\", type=str, default=\"test_vocab.pt\")\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def test(opt):\n",
        "    test_params = {\"batch_size\": opt.batch_size,\n",
        "                   \"shuffle\": False,\n",
        "                   \"drop_last\": True}\n",
        "    if os.path.isdir(opt.output):\n",
        "        shutil.rmtree(opt.output)\n",
        "    os.makedirs(opt.output)\n",
        "    if torch.cuda.is_available():\n",
        "        model = torch.load(opt.pre_trained_model)\n",
        "    else:\n",
        "        model = torch.load(opt.pre_trained_model, map_location=lambda storage, loc: storage)\n",
        "    \n",
        "    test_set = MyDataset(\n",
        "        opt.data_path, \n",
        "        opt.use_char_embeddings,\n",
        "        opt.char2vec_file,\n",
        "        opt.word2vec_file,\n",
        "        model.max_char_length,\n",
        "        model.max_word_length,\n",
        "        model.max_sent_length,\n",
        "        dataset_name='dataset', \n",
        "        min_freq=opt.min_freq,\n",
        "        pre_vocab=opt.pre_vocab)\n",
        "    #test_set = Subset(test_set,torch.LongTensor(random.sample(range(len(test_set)), opt.test_subset_len)))\n",
        "\n",
        "    test_generator = DataLoader(test_set, **test_params)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    te_label_ls = []\n",
        "    te_pred_ls = []\n",
        "    for te_feature, te_label in test_generator:\n",
        "        num_sample = len(te_label)\n",
        "        if torch.cuda.is_available():\n",
        "            te_feature = te_feature.cuda()\n",
        "            te_label = te_label.cuda()\n",
        "        with torch.no_grad():\n",
        "            model._init_hidden_state(num_sample)\n",
        "            te_predictions = model(te_feature)\n",
        "            te_predictions = F.softmax(te_predictions)\n",
        "        te_label_ls.extend(te_label.clone().cpu())\n",
        "        te_pred_ls.append(te_predictions.clone().cpu())\n",
        "    te_pred = torch.cat(te_pred_ls, 0).numpy()\n",
        "    te_label = np.array(te_label_ls)\n",
        "\n",
        "    fieldnames = ['True label', 'Predicted label', 'Content']\n",
        "    with open(opt.output + os.sep + \"predictions.csv\", 'w') as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames, quoting=csv.QUOTE_NONNUMERIC)\n",
        "        writer.writeheader()\n",
        "        for i, j, k in zip(te_label, te_pred, test_set.get_texts()):\n",
        "            writer.writerow(\n",
        "                {'True label': i + 1, 'Predicted label': np.argmax(j) + 1, 'Content': k})\n",
        "\n",
        "    test_metrics = get_evaluation(te_label, te_pred,\n",
        "                                  list_metrics=[\"accuracy\", \"loss\", \"confusion_matrix\"])\n",
        "    print(\"Prediction:\\nLoss: {} Accuracy: {} \\nConfusion matrix: \\n{}\".format(test_metrics[\"loss\"],\n",
        "                                                                               test_metrics[\"accuracy\"],\n",
        "                                                                               test_metrics[\"confusion_matrix\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN_pgTfYuoPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "    \"\"\"Implementation of the model described in the paper: Hierarchical Attention Networks for Document Classification\"\"\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--num_epoches\", type=int, default=1)\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
        "    parser.add_argument(\"--use_char_embeddings\", type=bool, default=False)\n",
        "    parser.add_argument(\"--char_hidden_size\", type=int, default=50)\n",
        "    parser.add_argument(\"--word_hidden_size\", type=int, default=50)\n",
        "    parser.add_argument(\"--sent_hidden_size\", type=int, default=50)\n",
        "    parser.add_argument(\"--es_min_delta\", type=float, default=0.0,\n",
        "                        help=\"Early stopping's parameter: minimum change loss to qualify as an improvement\")\n",
        "    parser.add_argument(\"--es_patience\", type=int, default=100,\n",
        "                        help=\"Early stopping's parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.\")\n",
        "    parser.add_argument(\"--train_set\", type=str, default=\"train.csv\")\n",
        "    parser.add_argument(\"--training_subset_percentage\", type=float, default=0.2,\n",
        "                        help=\"% of the total training data to be used by the model. Bear in mind the that this is related to the number of batch you can generate.\")\n",
        "    parser.add_argument(\"--val_subset_percentage\", type=float, default=0.2,\n",
        "                        help=\"% of the total val and validation data to be used by the model. Bear in mind the that this is related to the number of batch you can generate.\")\n",
        "    parser.add_argument(\"--val_interval\", type=int, default=1, help=\"Number of epoches between validation phases\")\n",
        "    parser.add_argument(\"--char2vec_file\", type=str, default=\"glove.6B.50d-char.txt\")\n",
        "    parser.add_argument(\"--word2vec_file\", type=str, default=\"glove.6B.50d\")\n",
        "    parser.add_argument(\"--log_path\", type=str, default=\"gdrive/My Drive/HAN_results/ag_news/tensorboard/han_voc\")\n",
        "    parser.add_argument(\"--saved_path\", type=str, default=\"gdrive/My Drive/HAN_results/ag_news/trained_models\")\n",
        "    parser.add_argument(\"--pre_trained_model\", type=str, default=None)\n",
        "    parser.add_argument(\"--max_length_char\", type=int,default=20)\n",
        "    parser.add_argument(\"--max_length_word\", type=int, default=25)  \n",
        "    parser.add_argument(\"--max_length_sentences\", type=int,default=12)\n",
        "    parser.add_argument(\"--min_freq\", type=int, default=6)\n",
        "    parser.add_argument(\"--pre_vocab\",type=int,default='train_vocab.pt')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def train(opt):\n",
        "    print('Starting...')\n",
        "    random.seed(123)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(123)\n",
        "    else:\n",
        "        torch.manual_seed(123)\n",
        "    print('Seed stablished')   \n",
        "    try:\n",
        "      os.makedirs(opt.saved_path)\n",
        "    except FileExistsError:\n",
        "      # directory already exists\n",
        "      pass  \n",
        "    output_file = open(opt.saved_path + os.sep + \"logs.txt\", \"w+\")\n",
        "    print('Output file created')\n",
        "    output_file.write(\"Model's parameters: {}\".format(vars(opt)))\n",
        "\n",
        "    print('Creating datasets')\n",
        "    training_set = MyDataset(\n",
        "        opt.train_set,\n",
        "        opt.use_char_embeddings,\n",
        "        opt.char2vec_file,\n",
        "        opt.word2vec_file,\n",
        "        opt.max_length_char,\n",
        "        opt.max_length_word,\n",
        "        opt.max_length_sentences,\n",
        "        dataset_name='dataset',\n",
        "        opt.min_freq, \n",
        "        opt.pre_vocab)\n",
        "    num_classes = training_set.num_classes\n",
        "    print(num_classes, 'Num classes')\n",
        "    training_set = Subset(training_set, torch.LongTensor(random.sample(range(len(training_set)), int(opt.training_subset_percentage * len(training_set) ) ) ) )\n",
        "    tr_cant = int(len(training_set) * 0.8)\n",
        "    training_set, val_set = random_split(training_set, [tr_cant, len(training_set) - tr_cant])\n",
        "\n",
        "    training_params = {\"batch_size\": opt.batch_size,\n",
        "                       \"drop_last\": True}\n",
        "    val_params = {\"batch_size\": opt.batch_size,\n",
        "                   \"drop_last\": True}\n",
        "\n",
        "    training_generator = DataLoader(training_set, **training_params)\n",
        "    print(training_generator.dataset)\n",
        "    print('Training generator created')\n",
        "    val_generator = DataLoader(val_set, **val_params)\n",
        "    print('Validation generator created')\n",
        "\n",
        "    '''\n",
        "    print('Creating datasets')\n",
        "    training_set = MyDataset(\n",
        "        opt.train_set,\n",
        "        opt.use_char_embeddings,\n",
        "        opt.char2vec_file,\n",
        "        opt.word2vec_file,\n",
        "        opt.max_length_char,\n",
        "        opt.max_length_word,\n",
        "        opt.max_length_sentences,\n",
        "        dataset_name='dataset',\n",
        "        opt.min_freq, \n",
        "        opt.pre_vocab)\n",
        "    num_classes = training_set.num_classes\n",
        "    print(num_classes, 'Num classes')\n",
        "    print('Training dataset created')\n",
        "    test_set = MyDataset(\n",
        "        data_path='ag_news_csv/test.csv',\n",
        "        opt.use_char_embeddings,\n",
        "        opt.char2vec_file,\n",
        "        opt.word2vec_file,\n",
        "        opt.max_length_char,\n",
        "        opt.max_length_word,\n",
        "        opt.max_length_sentences,\n",
        "        dataset_name='dataset',\n",
        "        opt.min_freq, \n",
        "        pre_vocab='ag_news_csv/test_vocab.pt')\n",
        "    print('Testing dataset created')\n",
        "\n",
        "    training_set_total_count = len(training_set)\n",
        "    subset_idx = torch.LongTensor(random.sample(range(training_set_total_count), int(1 * training_set_total_count)))\n",
        "    class_sample_count = torch.tensor([training_set.get_target_count(i) for i in range(num_classes)])\n",
        "    weight = 1. / class_sample_count.float()\n",
        "    samples_weight = torch.tensor([weight[t] for t in training_set.get_targets([str(int(t)) for t in subset_idx])])\n",
        "    training_sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "\n",
        "    test_set_total_count = len(test_set)\n",
        "    subset_idx = torch.LongTensor(random.sample(range(test_set_total_count), int(0.1 * test_set_total_count)))\n",
        "    class_sample_count = torch.tensor([test_set.get_target_count(i) for i in range(num_classes)])\n",
        "    weight = 1. / class_sample_count.float()\n",
        "    samples_weight = torch.tensor([weight[t] for t in test_set.get_targets([str(int(t)) for t in subset_idx])])\n",
        "    test_sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "\n",
        "    training_params = {\"batch_size\": opt.batch_size,\n",
        "                       \"sampler\": training_sampler,\n",
        "                       \"drop_last\": True}\n",
        "    test_params = {\"batch_size\": opt.batch_size,\n",
        "                   \"sampler\": test_sampler,\n",
        "                   \"drop_last\": False}\n",
        "\n",
        "    training_generator = DataLoader(training_set, **training_params)\n",
        "    print('Training generator created')\n",
        "    val_generator = DataLoader(test_set, **test_params)\n",
        "    print('Testing generator created')\n",
        "    '''\n",
        "\n",
        "    if opt.pre_trained_model is None:\n",
        "      model = HierAttNet(\n",
        "          opt.use_char_embeddings,\n",
        "          opt.char_hidden_size,\n",
        "          opt.word_hidden_size, \n",
        "          opt.sent_hidden_size,\n",
        "          opt.batch_size, \n",
        "          num_classes, \n",
        "          opt.char2vec_file,\n",
        "          opt.word2vec_file,\n",
        "          opt.max_length_char,\n",
        "          opt.max_length_word,\n",
        "          opt.max_length_sentences)\n",
        "    else:\n",
        "      model = torch.load(opt.pre_trained_model)\n",
        "\n",
        "    #if os.path.isdir(opt.log_path):\n",
        "    #    shutil.rmtree(opt.log_path)\n",
        "    #os.makedirs(opt.log_path)\n",
        "    writer = SummaryWriter(opt.log_path)\n",
        "    # writer.add_graph(model, torch.zeros(opt.batch_size, max_sent_length, max_word_length))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr, momentum=opt.momentum)\n",
        "    best_loss = 1e5\n",
        "    best_epoch = 0\n",
        "    model.train()\n",
        "    num_iter_per_epoch = len(training_generator)\n",
        "    for epoch in range(opt.num_epoches):\n",
        "        print(epoch, 'Epoch')\n",
        "        for iter, (feature, label) in enumerate(training_generator):\n",
        "            if torch.cuda.is_available():\n",
        "                feature = feature.cuda()\n",
        "                label = label.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            model._init_hidden_state()\n",
        "            predictions = model(feature)\n",
        "            loss = criterion(predictions, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"accuracy\"])\n",
        "            print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
        "                epoch + 1,\n",
        "                opt.num_epoches,\n",
        "                iter + 1,\n",
        "                num_iter_per_epoch,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                loss, training_metrics[\"accuracy\"]))\n",
        "            writer.add_scalar('Train/Loss', loss, epoch * num_iter_per_epoch + iter)\n",
        "            writer.add_scalar('Train/Accuracy', training_metrics[\"accuracy\"], epoch * num_iter_per_epoch + iter)\n",
        "        if epoch % opt.val_interval == 0:\n",
        "            model.eval()\n",
        "            loss_ls = []\n",
        "            te_label_ls = []\n",
        "            te_pred_ls = []\n",
        "            for te_feature, te_label in val_generator:\n",
        "                num_sample = len(te_label)\n",
        "                if torch.cuda.is_available():\n",
        "                    te_feature = te_feature.cuda()\n",
        "                    te_label = te_label.cuda()\n",
        "                with torch.no_grad():\n",
        "                    model._init_hidden_state(num_sample)\n",
        "                    te_predictions = model(te_feature)\n",
        "                te_loss = criterion(te_predictions, te_label)\n",
        "                loss_ls.append(te_loss * num_sample)\n",
        "                te_label_ls.extend(te_label.clone().cpu())\n",
        "                te_pred_ls.append(te_predictions.clone().cpu())\n",
        "            te_loss = sum(loss_ls) / val_set.__len__()\n",
        "            te_pred = torch.cat(te_pred_ls, 0)\n",
        "            te_label = np.array(te_label_ls)\n",
        "            val_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
        "            output_file.write(\n",
        "                \"Epoch: {}/{} \\nVal loss: {} Val accuracy: {} \\nVal confusion matrix: \\n{}\\n\\n\".format(\n",
        "                    epoch + 1, opt.num_epoches,\n",
        "                    te_loss,\n",
        "                    val_metrics[\"accuracy\"],\n",
        "                    val_metrics[\"confusion_matrix\"]))\n",
        "            print(val_metrics[\"confusion_matrix\"], 'test confusion')\n",
        "            print(\"Epoch: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
        "                epoch + 1,\n",
        "                opt.num_epoches,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                te_loss, val_metrics[\"accuracy\"]))\n",
        "            writer.add_scalar('Val/Loss', te_loss, epoch)\n",
        "            writer.add_scalar('Val/Accuracy', val_metrics[\"accuracy\"], epoch)\n",
        "            model.train()\n",
        "            if te_loss + opt.es_min_delta < best_loss:\n",
        "                best_loss = te_loss\n",
        "                best_epoch = epoch\n",
        "                torch.save(model, opt.saved_path + os.sep + \"whole_model_han\")\n",
        "\n",
        "            # Early stopping\n",
        "            if epoch - best_epoch > opt.es_patience > 0:\n",
        "                print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "681Q7tfOwWFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext.vocab as vocab\n",
        "\n",
        "vocab.pretrained_aliases['glove.6B.50d']()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SiPT9la3WaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training dataset to use\n",
        "train_path = 'ag_news_csv/train.csv'\n",
        "word2vec_file = 'glove.6B.50d'\n",
        "max_length_word = 25\n",
        "max_length_sentences = 12\n",
        "train_pre_vocab = 'ag_news_csv/train_vocab.pt'\n",
        "training_set_cat = MyDataset(\n",
        "    data_path=train_path, \n",
        "    word2vec_file=word2vec_file, \n",
        "    max_length_sentences=max_length_sentences,\n",
        "    max_length_word=max_length_word,\n",
        "    dataset_name='dataset',\n",
        "    min_freq=1, \n",
        "    pre_vocab=train_pre_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk4xylWWvW4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "#training\n",
        "\n",
        "batch_size_train = 64\n",
        "log_path_train = 'HAN_results/ag_news_1/trained_models'\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "train_epochs = 20\n",
        "train_saved_path = 'HAN_results/ag_news_1/trained_models'\n",
        "use_char_embeddings=True\n",
        "char_hidden_size=50\n",
        "word_hidden_size = 50\n",
        "sent_hidden_size = 50\n",
        "max_length_char = 20\n",
        "max_length_word = 25\n",
        "max_length_sentences = 12\n",
        "word2vec_file='glove.6B.50d'\n",
        "char2vec_file='glove.840B.300d-char.txt' \n",
        "training_subset_percentage=.9\n",
        "val_subset_percentage=.1\n",
        "train_min_freq = 1 \n",
        "train_pre_vocab = None #'ag_news_csv/train_vocab.pt'\n",
        "pre_trained_model_train = 'HAN_results/ag_news_1/trained_models/whole_model_han'\n",
        "\n",
        "#testing\n",
        "\n",
        "batch_size_test = 64\n",
        "test_data_path = 'ag_news_csv/test.csv' \n",
        "pre_trained_model_test = \"HAN_results/ag_news_1/trained_models/whole_model_han\"\n",
        "test_subset_len =  int(((len(training_set_cat) * training_subset_percentage) * val_subset_percentage))\n",
        "test_min_freq = 1\n",
        "test_pre_vocab = None #'ag_news_csv/test_vocab.pt'\n",
        "output = \"HAN_results/ag_news_1/trained_models/predictions\"\n",
        "\n",
        "print(test_subset_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtvxr-8yuw_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "%cd \"gdrive/My Drive\"\n",
        "train(Namespace(\n",
        "    batch_size=batch_size_train, \n",
        "    es_min_delta=0.0, \n",
        "    es_patience=5, \n",
        "    log_path=log_path_train, \n",
        "    lr=lr, \n",
        "    momentum=momentum, \n",
        "    num_epoches=train_epochs, \n",
        "    saved_path=train_saved_path,\n",
        "    use_char_embeddings=use_char_embeddings,\n",
        "    char_hidden_size=char_hidden_size,\n",
        "    word_hidden_size=word_hidden_size, \n",
        "    sent_hidden_size=sent_hidden_size,\n",
        "    val_interval=1, \n",
        "    train_set=train_path,\n",
        "    char2vec_file=char2vec_file,\n",
        "    word2vec_file=word2vec_file, \n",
        "    training_subset_percentage=training_subset_percentage, \n",
        "    val_subset_percentage=val_subset_percentage,\n",
        "    pre_trained_model=pre_trained_model_train,\n",
        "    max_length_char=max_length_char,\n",
        "    max_length_word=max_length_word,\n",
        "    max_length_sentences=max_length_sentences, \n",
        "    min_freq=train_min_freq,\n",
        "    pre_vocab=train_pre_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZGfutyHuzL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "%cd \"gdrive/My Drive\"\n",
        "test(Namespace(\n",
        "    batch_size=batch_size_test, \n",
        "    data_path=test_data_path,\n",
        "    pre_trained_model=pre_trained_model_test,\n",
        "    user_char_embeddings=use_char_embeddings,\n",
        "    char2vec_file=char2vec_file, \n",
        "    word2vec_file=word2vec_file,\n",
        "    output=output,\n",
        "    test_subset_len=test_subset_len,\n",
        "    min_freq=test_min_freq,\n",
        "    pre_vocab=test_pre_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}